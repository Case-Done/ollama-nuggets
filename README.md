# ollama-nuggets
Coding nuggets about using Ollama and its LLM models

## Context Length Experiments

This repository contains experiments exploring Ollama's behavior with different context and prediction length settings, using the llama3.2 model.

### Key Findings

- Demonstrated how to use different context lengths (`num_ctx`) and prediction lengths (`num_predict`) with Ollama
- Compared output quality between default and expanded context settings
- Explored the relationship between prediction length and output completeness
- Analyzed token counts and model behavior under different configurations

## Setup
```shell
pip install -r requirements.txt
```