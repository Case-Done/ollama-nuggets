{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaLLM as Ollama\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample-text.txt\", \"r\") as file:\n",
    "    transcript = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"think it's possible that physics has exploits and we should be trying to \"\n",
      " 'find them arranging some kind of a crazy quantum mechanical system that '\n",
      " 'somehow gives you buffer overflow somehow gives you a rounding error in the '\n",
      " 'floating Point synthetic intelligences are kind of like the next stage of '\n",
      " \"development and I don't know where it leads to like at some point I suspect \"\n",
      " 'the universe is some kind of a puzzle these synthetic AIS will uncover that '\n",
      " 'puzzle and solve it the following is a conversation w')\n"
     ]
    }
   ],
   "source": [
    "pprint(transcript[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_splitter(chunk_size: int, overlap_size: int):\n",
    "    return RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size, chunk_overlap=overlap_size)\n",
    "\n",
    "def convert_text_to_tokens(text, encoder=\"gpt-3.5-turbo\"):\n",
    "    enc = tiktoken.encoding_for_model(encoder)\n",
    "    return enc.encode(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare docs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only used 50000 letters from the `transcript` variable, so we can use the same chunk size for all docs\n",
    "chunk_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs:  4\n",
      "Characters counts per document: [12718, 12370, 12612, 12297]\n",
      "Total Characters across 4 documents: 49997\n",
      "Token counts per document: [2478, 2505, 2488, 2395]\n",
      "Total tokens across 4 documents: 9866\n",
      "Word counts per document: [2313, 2332, 2314, 2269]\n",
      "Total words across 4 documents: 9228\n"
     ]
    }
   ],
   "source": [
    "transcript_up_to = 50000\n",
    "\n",
    "chunk_size = 2500 # this is in tokens\n",
    "overlap_size = 0 # this is in tokens\n",
    "\n",
    "### CODE\n",
    "\n",
    "docs = [Document(\n",
    "    page_content=transcript[:transcript_up_to],\n",
    "    # metadata={\"source\": url}\n",
    ")]\n",
    "\n",
    "text_splitter = get_text_splitter(chunk_size=chunk_size, overlap_size=overlap_size)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "n_docs = len(split_docs)\n",
    "\n",
    "print(\"number of docs: \", n_docs)\n",
    "\n",
    "# Count characters in each document\n",
    "char_counts = [len(doc.page_content) for doc in split_docs]\n",
    "print(f\"Characters counts per document: {char_counts}\")\n",
    "print(f\"Total Characters across {n_docs} documents: {sum(char_counts)}\")\n",
    "# split_docs\n",
    "  # Using gpt-3.5-turbo tokenizer as closest approximation for llama models\n",
    "token_counts = [len(convert_text_to_tokens(doc.page_content)) for doc in split_docs]\n",
    "print(f\"Token counts per document: {token_counts}\")\n",
    "print(f\"Total tokens across {n_docs} documents: {sum(token_counts)}\")\n",
    "# Count words in each document\n",
    "word_counts = [len(doc.page_content.split()) for doc in split_docs]\n",
    "print(f\"Word counts per document: {word_counts}\")\n",
    "print(f\"Total words across {n_docs} documents: {sum(word_counts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2\"\n",
    "base_url = \"http://localhost:11434\"\n",
    "temperature = 0.5\n",
    "num_ctx = 1024 # number of context length, DEFAULT = 2048\n",
    "# https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#langchain_ollama.llms.OllamaLLM.num_ctx\n",
    "num_predict = 128 # number of tokens to predict, Default: 128, -1 = infinite generation, -2 = fill context\n",
    "# https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#langchain_ollama.llms.OllamaLLM.num_predict\n",
    "# more info: https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#ollamallm\n",
    "\n",
    "llm = Ollama(\n",
    "        model=model,\n",
    "        base_url=base_url,\n",
    "        temperature=temperature,\n",
    "        num_ctx=num_ctx,\n",
    "        num_predict=num_predict,\n",
    "        format='',\n",
    "        verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens: 2506\n",
      "\n",
      "('Write a detail summary of this text section in bullet points. Answer just '\n",
      " 'the bullet points, no other text.\\n'\n",
      " 'Text:\\n'\n",
      " \"think it's possible that physics has exploits and we should be trying to \"\n",
      " 'find them arranging some kind of a crazy quantum mechanical system that '\n",
      " 'somehow gives you buffer overflow somehow gives you a rounding error in the '\n",
      " 'floating Point synthetic intelligences are kind of like the next stage of '\n",
      " \"development and I don't know where it leads to like at some point I suspect \"\n",
      " 'the universe is some kind of a puzzle these synthetic AIS will uncover that '\n",
      " 'puzzle and solve it the following is a conversation with Andre capothy '\n",
      " 'previously the director of AI at Tesla and before that at open Ai and '\n",
      " 'Stanford he is one of the greatest scientists engineers and Educators in the '\n",
      " 'history of artificial intelligence this is the Lex Friedman podcast to '\n",
      " \"support it please check out our sponsors and now dear friends here's Andre \"\n",
      " 'capathi what is a neural network and why does it seem to uh do such a '\n",
      " \"surprisingly good job of learning what is a neural network it's a \"\n",
      " \"mathematical abstraction of the brain I would say that's how it was \"\n",
      " \"originally developed at the end of the day it's a mathematical expression \"\n",
      " \"and it's a fairly simple mathematical expression when you get down to it \"\n",
      " \"it's basically a sequence of Matrix multiplies which are really dot products \"\n",
      " \"mathematically and some nonlinearities thrown in and so it's a very simple \"\n",
      " \"mathematical expression and it's got knobs in it many knobs many knobs and \"\n",
      " 'these knobs are Loosely related to basically the synapses in your brain '\n",
      " \"they're trainable they're modifiable and so the idea is like we need to find \"\n",
      " 'the setting of The Knobs that makes the neural nut do whatever you want it '\n",
      " \"to do like classify images and so on and so there's not too much mystery I \"\n",
      " \"would say in it like um you might think that basically don't want to endow \"\n",
      " \"it with too much meaning with respect to the brain and how it works it's \"\n",
      " 'really just a complicated mathematical expression with knobs and those knobs '\n",
      " 'need a proper setting for it to do something uh desirable yeah but poetry is '\n",
      " 'just the collection of letters with spaces but it can make us feel a certain '\n",
      " 'way and in that same way when you get a large number of knobs together '\n",
      " \"whether it's in a inside the brain or inside a computer they seem to they \"\n",
      " \"seem to surprise us with the with their power yeah I think that's fair so \"\n",
      " \"basically I'm underselling it by a lot because you definitely do get very \"\n",
      " \"surprising emergent behaviors out of these neurons when they're large enough \"\n",
      " 'and trained on complicated enough problems like say for example the next uh '\n",
      " 'word prediction in a massive data set from the internet and then these '\n",
      " \"neurons take on a pretty surprising magical properties yeah I think it's \"\n",
      " 'kind of interesting how much you can get out of even very simple '\n",
      " 'mathematical formalism when your brain right now I was talking is it doing '\n",
      " 'next word prediction or is it doing something more interesting well '\n",
      " \"definitely some kind of a generative model that's a gpt-like and prompted by \"\n",
      " \"you um yeah so you're giving me a prompt and I'm kind of like responding to \"\n",
      " 'it in a generative way and by yourself perhaps a little bit like are you '\n",
      " 'adding extra prompts from your own memory inside your head automatically '\n",
      " \"feels like you're referencing some kind of a declarative structure of like \"\n",
      " \"memory and so on and then uh you're putting that together with your prompt \"\n",
      " 'and giving away some messages like how much of what you just said has been '\n",
      " 'said by you before uh nothing basically right no but if you actually look at '\n",
      " \"all the words you've ever said in your life and you do a search you'll \"\n",
      " 'probably said a lot of the same words in the same order before yeah it could '\n",
      " \"be I mean I'm using phrases that are common Etc but I'm remixing it into a \"\n",
      " \"pretty uh sort of unique sentence at the end of the day but you're right \"\n",
      " \"definitely there's like a ton of remixing what you didn't you it's like \"\n",
      " \"Magnus Carlsen said uh I'm I'm rated 2900 whatever which is pretty decent I \"\n",
      " \"think you're talking very uh you're not giving enough credit to neural Nets \"\n",
      " \"here why do they seem to what's your best intuition about this emergent \"\n",
      " \"Behavior I mean it's kind of interesting because I'm simultaneously \"\n",
      " \"underselling them but I also feel like there's an element to which I'm over \"\n",
      " \"like it's actually kind of incredible that you can get so much emergent \"\n",
      " 'magical Behavior out of them despite them being so simple mathematically so '\n",
      " 'I think those are kind of like two surprising statements that are kind of '\n",
      " 'just juxtapose together and I think basically what it is is we are actually '\n",
      " 'fairly good at optimizing these neural Nets and when you give them a hard '\n",
      " 'enough problem they are forced to learn very interesting Solutions in the '\n",
      " 'optimization and those solution basically have these immersion properties '\n",
      " \"that are very interesting there's wisdom and knowledge in the knobs and so \"\n",
      " \"what's this representation that's in the knobs does it make sense to you \"\n",
      " 'intuitively the large number of knobs can hold the representation that '\n",
      " \"captures some deep wisdom about the data it has looked at it's a lot of \"\n",
      " \"knobs it's a lot of knobs and somehow you know so speaking concretely um one \"\n",
      " 'of the neural Nets that people are very excited about right now are are gpts '\n",
      " 'which are basically just next word prediction networks so you consume a '\n",
      " 'sequence of words from the internet and you try to predict the next word and '\n",
      " 'uh once you train these on a large enough data set um they you can basically '\n",
      " 'uh prompt these neural amounts in arbitrary ways and you can ask them to '\n",
      " 'solve problems and they will so you can just tell them you can you can make '\n",
      " \"it look like you're trying to um solve some kind of a mathematical problem \"\n",
      " \"and they will continue what they think is the solution based on what they've \"\n",
      " 'seen on the internet and very often those Solutions look very remarkably '\n",
      " 'consistent look correct potentially do you still think about the brain side '\n",
      " 'of it so as neural Nets is an abstraction or mathematical abstraction of the '\n",
      " 'brain you still draw wisdom from from the biological neural networks or even '\n",
      " \"the bigger question so you're a big fan of biology and biological \"\n",
      " 'computation what impressive thing is biology do doing to you that computers '\n",
      " \"are not yet that Gap I would say I'm definitely on I'm much more hesitant \"\n",
      " 'with the analogies to the brain than I think you would see potentially in '\n",
      " 'the field um and I kind of feel like certainly the way neural network '\n",
      " 'started is everything stemmed from inspiration by the brain but at the end '\n",
      " 'of the day the artifacts that you get after training they are arrived at by '\n",
      " 'a very different optimization process than the optimization process that '\n",
      " 'gave rise to the brain and so I think uh I kind of think of it as a very '\n",
      " \"complicated alien artifact um it's something different I'm not sorry the uh \"\n",
      " \"the neuralness that we're training okay they are complicated uh Alien \"\n",
      " 'artifact uh I do not make analogies to the brain because I think the '\n",
      " 'optimization process that gave rise to it is very different from the brain '\n",
      " 'so there was no multi-agent self-play kind of uh setup uh and evolution it '\n",
      " 'was an optimization that is basically a what amounts to a compression '\n",
      " 'objective on a massive amount of data okay so artificial neural networks are '\n",
      " 'doing compression and biological neural networks are not to survive and '\n",
      " \"they're not really doing any they're they're an agent in a multi-agent \"\n",
      " \"self-place system that's been running for a very very long time that said \"\n",
      " 'Evolution has found that it is very useful to to predict and have a '\n",
      " 'predictive model in the brain and so I think our brain utilizes something '\n",
      " 'that looks like that as a part of it but it has a lot more you know gadgets '\n",
      " 'and gizmos and uh value functions and ancient nuclei that are all trying to '\n",
      " 'like make a survive and reproduce and everything else and the whole thing '\n",
      " \"through embryogenesis is built from a single cell I mean it's just the code \"\n",
      " 'is inside the DNA and it just builds it up like the entire organism yes and '\n",
      " \"like it does it pretty well it should not be possible so there's some \"\n",
      " \"learning going on there's some there's some there's some kind of computation \"\n",
      " \"going through that building process I mean I I don't know where if you were \"\n",
      " 'just to look at the entirety of history of life on Earth where do you think '\n",
      " 'is the most interesting invention is it the origin of life itself is it just '\n",
      " 'jumping to eukaryotes is it mammals is it humans themselves Homo sapiens the '\n",
      " 'the origin of intelligence or highly complex intelligence or or is it all '\n",
      " \"just in continuation the same kind of process certainly I would say it's an \"\n",
      " \"extremely remarkable story that I'm only like briefly learning about \"\n",
      " 'recently all the way from um actually like you almost have to start at the '\n",
      " 'formation of Earth and all of its conditions and the entire solar system and '\n",
      " 'how everything is arranged with Jupiter and Moon and the habitable zone and '\n",
      " \"everything and then you have an active Earth that's turning over material \"\n",
      " \"and um and then you start with a biogenesis and everything and so it's all \"\n",
      " \"like a pretty remarkable story I'm not sure that I can pick like a single \"\n",
      " 'Unique Piece of it that I find most interesting um I guess for me as an '\n",
      " \"artificial intelligence researcher it's probably the last piece we have lots \"\n",
      " 'of animals that uh you know are are not building technological Society but '\n",
      " 'we do and um it seems to have happened very quickly it seems to have '\n",
      " 'happened very recently and uh something very interesting happened there that '\n",
      " \"I don't fully understand I almost understand everything else kind of I think \"\n",
      " \"intuitively uh but I don't understand exactly that part and how quick it was \"\n",
      " 'both explanations would be interesting one is that this is just a '\n",
      " \"continuation of the same kind of process there's nothing special about \"\n",
      " 'humans that would be deeply understanding that would be very interesting '\n",
      " 'that we think of ourselves as special but it was obvious all it was already '\n",
      " 'written in the in the code that you would have greater and greater '\n",
      " 'intelligence emerging and then the other explanation which is something '\n",
      " \"truly special happened something like a rare event whether it's like crazy \"\n",
      " 'rare event like uh Space Odyssey what would it be see if you say like the '\n",
      " 'invention of Fire or the uh as Richard rangham says the beta males deciding '\n",
      " 'a clever way to kill the alpha males by collaborating so just optimizing the '\n",
      " 'collaborations really the multi-agent aspect of the multi-agent and that '\n",
      " 'really being constrained on resources and trying to survive the '\n",
      " 'collaboration aspect is what created the complex intelligence but it seems '\n",
      " \"like it's a natural outgrowth of the evolution process like what could \"\n",
      " 'possibly be a magical thing that happened like a rare thing that would say '\n",
      " 'that humans are actually human level intelligence is actually a really rare '\n",
      " \"thing in the universe yeah I'm hesitant to say that it is rare by the way \"\n",
      " \"but it definitely seems like it's kind of like a punctuated equilibrium \"\n",
      " 'where you have lots of exploration and then you have certain leaps sparse '\n",
      " 'leaps in between so of course like origin of life would be one um you know '\n",
      " 'DNA sex eukaryotic system eukaryotic life um the endosymbiosis event or the '\n",
      " 'archaeon 8 little bacteria you know just the whole thing and then of course '\n",
      " 'emergence of Consciousness and so on so it seems like definitely there are '\n",
      " \"sparse events where mass amount of progress was made but yeah it's kind of \"\n",
      " \"hard to pick one so you don't think humans are unique gotta ask you how many \"\n",
      " 'intelligent aliens civilizations do you think are out there and uh is there '\n",
      " \"intelligence different or similar to ours yeah I've been preoccupied with \"\n",
      " 'this question quite a bit recently uh basically the for me Paradox and just '\n",
      " 'thinking through and and the reason actually that I am very interested in uh '\n",
      " 'the origin of life is fundamentally trying to understand how common it is '\n",
      " 'that there are technological societies out there uh um in space and the more '\n",
      " 'I study it the more I think that um uh there should be quite a few quite a '\n",
      " \"lot why haven't we heard from them because I I agree with you it feels like \"\n",
      " \"I just don't see why what we did here on Earth is so difficult to do yeah \"\n",
      " 'and especially when you get into the details of it I used to think origin of '\n",
      " 'life was very um it was this magical rare event but then you read books like '\n",
      " 'for example McLean um uh the vital question a life ascending Etc and he '\n",
      " 'really gets in and he really makes you believe that this is not that rare '\n",
      " 'basic chemistry you have an active Earth and you have your alkaline Vents '\n",
      " \"and you have lots of alkaline Waters mixing whether it's a devotion and you \"\n",
      " 'have your proton gradients and you have the little porous pockets of these '\n",
      " 'alkaline vents that concentrate chemistry and um basically as he steps '\n",
      " 'through all of these little pieces you start to understand\\n'\n",
      " '\\n'\n",
      " 'SUMMARY:')\n"
     ]
    }
   ],
   "source": [
    "MAP_TEMPLATE_TXT = \"\"\"Write a detail summary of this text section in bullet points. Answer just the bullet points, no other text.\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "SUMMARY:\"\"\"\n",
    "\n",
    "full_prompt = PromptTemplate(\n",
    "    template=MAP_TEMPLATE_TXT,\n",
    "    input_variables=[\"text\"]\n",
    ").format(\n",
    "    text=split_docs[0].page_content\n",
    ")\n",
    "\n",
    "print(\"number of tokens: {}\\n\".format(len(convert_text_to_tokens(full_prompt))))\n",
    "pprint(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1: Overflowing the context\n",
    "\n",
    "**Observe in Ollama that warning is thrown at us, saying \"truncating input prompt\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The speaker is discussing the origin of life on Earth and its potential '\n",
      " 'implications for the existence of intelligent alien civilizations. They '\n",
      " 'suggest that the emergence of complex intelligence may be a rare event, but '\n",
      " 'not necessarily unique to humans.\\n'\n",
      " '\\n'\n",
      " 'Key points:\\n'\n",
      " '\\n'\n",
      " '* The origin of life on Earth may have been more common than previously '\n",
      " 'thought.\\n'\n",
      " '* Basic chemistry can lead to the emergence of complex life forms under the '\n",
      " 'right conditions.\\n'\n",
      " '* The speaker is interested in understanding how common intelligent '\n",
      " \"societies are in the universe and why we haven't encountered any yet.\\n\"\n",
      " '* They believe that what we did on Earth, creating a technological society, '\n",
      " 'may not be as difficult as it seems.\\n'\n",
      " '\\n'\n",
      " 'The')\n"
     ]
    }
   ],
   "source": [
    "output = llm.invoke(full_prompt)\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Within the context length\n",
    "\n",
    "We will increase the context length of Ollama with `num_ctx` to 3072 (1024*3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('• A conversation between Andre Capathi, previously director of AI at Tesla '\n",
      " 'and OpenAI, and Lex Friedman about neural networks.\\n'\n",
      " '• Neural networks are a mathematical abstraction of the brain and use simple '\n",
      " 'mathematical expressions with knobs that can be trained to learn complex '\n",
      " 'behaviors.\\n'\n",
      " '• Despite being simple mathematically, neural networks produce surprising '\n",
      " 'emergent behaviors when used for tasks like next word prediction in massive '\n",
      " 'data sets.\\n'\n",
      " '• The conversation touches on the idea that optimizing neural networks '\n",
      " 'allows them to discover interesting solutions to problems.\\n'\n",
      " '• Capathi mentions GPT-like models and their ability to solve problems by '\n",
      " 'prompting them with arbitrary inputs.\\n'\n",
      " '• He also discusses the biological side of')\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.2\"\n",
    "base_url = \"http://localhost:11434\"\n",
    "temperature = 0.5\n",
    "num_ctx = 1024*3 # number of context length, DEFAULT = 2048\n",
    "# https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#langchain_ollama.llms.OllamaLLM.num_ctx\n",
    "num_predict = 128 # number of tokens to predict, Default: 128, -1 = infinite generation, -2 = fill context\n",
    "# https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#langchain_ollama.llms.OllamaLLM.num_predict\n",
    "# more info: https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#ollamallm\n",
    "\n",
    "llm = Ollama(\n",
    "        model=model,\n",
    "        base_url=base_url,\n",
    "        temperature=temperature,\n",
    "        num_ctx=num_ctx,\n",
    "        num_predict=num_predict,\n",
    "        format='',\n",
    "        verbose=True\n",
    ")\n",
    "\n",
    "output = llm.invoke(full_prompt)\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================= Output 1 ==========================\n",
      "('• Neural networks are mathematical abstractions of the brain, originally '\n",
      " 'developed as a simple mathematical expression with knobs (synapses) that can '\n",
      " 'be trained and modified.\\n'\n",
      " '• They seem to surprise us with their power and emergent behaviors, despite '\n",
      " 'being mathematically simple.\\n'\n",
      " '• The optimization process used to train neural networks is different from '\n",
      " 'the biological processes that give rise to the brain.\\n'\n",
      " '• Biological neural networks are part of a multi-agent system that has '\n",
      " 'evolved over time through evolution and self-play.\\n'\n",
      " '• The origin of intelligence in humans may be considered a punctuated '\n",
      " 'equilibrium event, with sparse leaps in progress.\\n'\n",
      " \"• It's difficult to determine if humans are unique\")\n",
      "\n",
      "======================= Output 2 ==========================\n",
      "('• A conversation with Andre Capathi, previously director of AI at Tesla and '\n",
      " 'OpenAI, Stanford, discusses neural networks.\\n'\n",
      " '• Neural networks are a mathematical abstraction of the brain, using simple '\n",
      " 'mathematical expressions with knobs (trainable parameters) to learn and '\n",
      " 'solve complex problems.\\n'\n",
      " '• The conversation touches on emergent behavior in neural networks, which '\n",
      " 'can lead to surprising and powerful results despite their simplicity '\n",
      " 'mathematically.\\n'\n",
      " '• Capathi attributes this emergence to optimizing these networks for large, '\n",
      " 'hard problems, leading to interesting solutions that exhibit \"magical '\n",
      " 'properties.\"\\n'\n",
      " '• He emphasizes the importance of biology and biological computation but '\n",
      " 'notes that the optimization process used in artificial neural networks')\n",
      "\n",
      "======================= Output 3 ==========================\n",
      "('• Neural networks are mathematical abstractions of the brain, originally '\n",
      " 'developed at the end of the day as a sequence of matrix multiplies with '\n",
      " 'nonlinearities.\\n'\n",
      " '• The knobs in neural networks are trainable and modifiable, allowing them '\n",
      " 'to learn and adapt to new data.\\n'\n",
      " '• Large neural networks can exhibit surprising emergent behaviors, such as '\n",
      " 'next word prediction or generative models.\\n'\n",
      " '• The optimization process that gives rise to neural networks is different '\n",
      " \"from the brain's optimization process, which involves multi-agent self-play \"\n",
      " 'and evolution.\\n'\n",
      " '• Biological neural networks are not just simple abstractions but complex '\n",
      " 'systems with many gadgets, gizmos, and value functions.\\n'\n",
      " '• The origin')\n",
      "\n",
      "======================= Output 4 ==========================\n",
      "('• A mathematical abstraction of the brain, neural networks are used for '\n",
      " 'learning and classification tasks.\\n'\n",
      " '• The structure of a neural network is similar to the synapses in the brain, '\n",
      " 'with knobs that can be adjusted to optimize performance.\\n'\n",
      " '• Despite being simple mathematically, neural networks exhibit surprising '\n",
      " 'emergent behavior when trained on large datasets.\\n'\n",
      " '• Optimization processes in neural networks differ from those in biological '\n",
      " 'brains, making them distinct \"alien artifacts\".\\n'\n",
      " '• The development of artificial intelligence is seen as a continuation of '\n",
      " 'the evolution process, with human-level intelligence possibly emerging '\n",
      " 'through natural selection.\\n'\n",
      " '• The origin of life and emergence of complex intelligence are considered '\n",
      " 'key events in the')\n",
      "\n",
      "======================= Output 5 ==========================\n",
      "('• A neural network is a mathematical abstraction of the brain, originally '\n",
      " \"developed at the end of the day it's a sequence of matrix multiplies (dot \"\n",
      " 'products) with nonlinearities.\\n'\n",
      " '• The idea is to find the setting of knobs that makes the neural network do '\n",
      " 'something desirable like classify images.\\n'\n",
      " '• Neural networks have surprising emergent behaviors when given large '\n",
      " 'problems and trained on complicated data sets.\\n'\n",
      " '• They can take on magical properties, such as generating text or solving '\n",
      " 'problems in a generative way.\\n'\n",
      " '• The optimization process used to train neural networks is different from '\n",
      " \"the brain's optimization process.\\n\"\n",
      " '• The brain uses multi-agent self-play and evolution to optimize')\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for i in range(5):\n",
    "    print(\"\\n======================= Output {} ==========================\".format(i+1))\n",
    "    _output = llm.invoke(full_prompt)\n",
    "    pprint(_output)\n",
    "    outputs.append(_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3: Increase prediction length\n",
    "To increase the chance of AI completing its answer, let's increase the prediction length with `num_pred` to 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2\"\n",
    "base_url = \"http://localhost:11434\"\n",
    "temperature = 0.5\n",
    "num_ctx = 1024*3 # number of context length, DEFAULT = 2048\n",
    "# https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#langchain_ollama.llms.OllamaLLM.num_ctx\n",
    "num_predict = 1024 # number of tokens to predict, Default: 128, -1 = infinite generation, -2 = fill context\n",
    "# https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#langchain_ollama.llms.OllamaLLM.num_predict\n",
    "# more info: https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#ollamallm\n",
    "\n",
    "llm = Ollama(\n",
    "        model=model,\n",
    "        base_url=base_url,\n",
    "        temperature=temperature,\n",
    "        num_ctx=num_ctx,\n",
    "        num_predict=num_predict,\n",
    "        format='',\n",
    "        verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('• A conversation with Andre Capathi, previously director of AI at Tesla and '\n",
      " 'OpenAI, discusses neural networks.\\n'\n",
      " '• Neural networks are a mathematical abstraction of the brain, using a '\n",
      " 'sequence of matrix multiplies and nonlinearities to learn and classify '\n",
      " 'images.\\n'\n",
      " '• The knobs in neural networks are trainable and modifiable, allowing for '\n",
      " 'optimization and emergent behavior.\\n'\n",
      " '• Large-scale training can lead to surprising magical properties in these '\n",
      " 'networks.\\n'\n",
      " '• GPT-like models have been trained on massive datasets and can solve '\n",
      " 'problems with remarkable consistency.\\n'\n",
      " '• Andre Capathi emphasizes that the optimization process giving rise to '\n",
      " \"neural networks is different from the brain's optimization process.\\n\"\n",
      " '• He notes that biological neural networks are complex and multi-agent '\n",
      " 'systems, whereas artificial neural networks are simpler mathematical '\n",
      " 'expressions.\\n'\n",
      " '• The origin of life and emergence of intelligence are seen as key questions '\n",
      " 'in understanding the complexity of intelligent systems.\\n'\n",
      " '• Andre Capathi believes that the origin of human-level intelligence may be '\n",
      " 'a rare event, but not necessarily unique to Earth.\\n'\n",
      " '• He is interested in the possibility of intelligent alien civilizations and '\n",
      " 'thinks there should be many technological societies out there.')\n"
     ]
    }
   ],
   "source": [
    "output = llm.invoke(full_prompt)\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with GPT-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../secret/openai_api_key.txt\", \"r\") as file:\n",
    "    openai_api_key = file.read()\n",
    "\n",
    "# openai.api_key = openai_api_key\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    api_key=openai_api_key,\n",
    ")\n",
    "\n",
    "output_gpt = chat_llm.invoke(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('- The text discusses the potential exploits in physics and the idea of '\n",
      " 'synthetic intelligences uncovering universal puzzles.\\n'\n",
      " '- It features a conversation with Andre Karpathy, former director of AI at '\n",
      " 'Tesla and OpenAI, on the nature of neural networks.\\n'\n",
      " '- Neural networks are described as mathematical abstractions of the brain, '\n",
      " 'primarily consisting of matrix multiplications and nonlinearities.\\n'\n",
      " '- The \"knobs\" in neural networks are likened to synapses in the brain and '\n",
      " 'are adjustable to achieve desired outcomes, such as image classification.\\n'\n",
      " '- Despite their mathematical simplicity, neural networks can exhibit '\n",
      " 'surprising emergent behaviors when trained on complex problems.\\n'\n",
      " '- The discussion touches on the generative capabilities of models like GPT, '\n",
      " 'which predict the next word based on large datasets.\\n'\n",
      " '- Karpathy expresses skepticism about drawing direct analogies between '\n",
      " 'artificial neural networks and biological brains due to different '\n",
      " 'optimization processes.\\n'\n",
      " '- He emphasizes that biological neural networks operate within a multi-agent '\n",
      " 'self-play system influenced by evolution, while artificial networks focus on '\n",
      " 'compression of data.\\n'\n",
      " '- The conversation explores significant milestones in the history of life on '\n",
      " 'Earth, such as the origin of life, eukaryotes, and human intelligence.\\n'\n",
      " '- Karpathy reflects on the rapid emergence of human technological societies '\n",
      " 'and the possible explanations for this phenomenon, including the idea of '\n",
      " 'unique events versus a natural progression of intelligence.\\n'\n",
      " '- He is curious about the existence of intelligent alien civilizations and '\n",
      " 'the reasons for the lack of contact with them, suggesting that the origin of '\n",
      " 'life may not be as rare as previously thought.')\n"
     ]
    }
   ],
   "source": [
    "pprint(output_gpt.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: space-between; width: 100%;\">\n",
       "    <div style=\"width: 48%; padding: 10px; border: 1px solid black; overflow-y: auto; height: 400px;\">\n",
       "        <h1>Ollama</h1>\n",
       "<ul>\n",
       "<li>A conversation with Andre Capathi, previously director of AI at Tesla and OpenAI, discusses neural networks.</li>\n",
       "<li>Neural networks are a mathematical abstraction of the brain, using a sequence of matrix multiplies and nonlinearities to learn and classify images.</li>\n",
       "<li>The knobs in neural networks are trainable and modifiable, allowing for optimization and emergent behavior.</li>\n",
       "<li>Large-scale training can lead to surprising magical properties in these networks.</li>\n",
       "<li>GPT-like models have been trained on massive datasets and can solve problems with remarkable consistency.</li>\n",
       "<li>Andre Capathi emphasizes that the optimization process giving rise to neural networks is different from the brain's optimization process.</li>\n",
       "<li>He notes that biological neural networks are complex and multi-agent systems, whereas artificial neural networks are simpler mathematical expressions.</li>\n",
       "<li>The origin of life and emergence of intelligence are seen as key questions in understanding the complexity of intelligent systems.</li>\n",
       "<li>Andre Capathi believes that the origin of human-level intelligence may be a rare event, but not necessarily unique to Earth.</li>\n",
       "<li>He is interested in the possibility of intelligent alien civilizations and thinks there should be many technological societies out there.</li>\n",
       "</ul>\n",
       "    </div>\n",
       "    <div style=\"width: 48%; padding: 10px; border: 1px solid black; overflow-y: auto; height: 400px;\">\n",
       "        <h1>GPT-4o-mini</h1>\n",
       "<ul>\n",
       "<li>The text discusses the potential exploits in physics and the idea of synthetic intelligences uncovering universal puzzles.</li>\n",
       "<li>It features a conversation with Andre Karpathy, former director of AI at Tesla and OpenAI, on the nature of neural networks.</li>\n",
       "<li>Neural networks are described as mathematical abstractions of the brain, primarily consisting of matrix multiplications and nonlinearities.</li>\n",
       "<li>The \"knobs\" in neural networks are likened to synapses in the brain and are adjustable to achieve desired outcomes, such as image classification.</li>\n",
       "<li>Despite their mathematical simplicity, neural networks can exhibit surprising emergent behaviors when trained on complex problems.</li>\n",
       "<li>The discussion touches on the generative capabilities of models like GPT, which predict the next word based on large datasets.</li>\n",
       "<li>Karpathy expresses skepticism about drawing direct analogies between artificial neural networks and biological brains due to different optimization processes.</li>\n",
       "<li>He emphasizes that biological neural networks operate within a multi-agent self-play system influenced by evolution, while artificial networks focus on compression of data.</li>\n",
       "<li>The conversation explores significant milestones in the history of life on Earth, such as the origin of life, eukaryotes, and human intelligence.</li>\n",
       "<li>Karpathy reflects on the rapid emergence of human technological societies and the possible explanations for this phenomenon, including the idea of unique events versus a natural progression of intelligence.</li>\n",
       "<li>He is curious about the existence of intelligent alien civilizations and the reasons for the lack of contact with them, suggesting that the origin of life may not be as rare as previously thought.</li>\n",
       "</ul>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import markdown\n",
    "\n",
    "# Convert Markdown to HTML\n",
    "html_text1 = markdown.markdown(\"# Ollama \\n\"+output.replace(\"•\",\"-\"))\n",
    "html_text2 = markdown.markdown(\"# GPT-4o-mini \\n\"+output_gpt.content)\n",
    "\n",
    "# Combine into side-by-side layout\n",
    "html_content = f\"\"\"\n",
    "<div style=\"display: flex; justify-content: space-between; width: 100%;\">\n",
    "    <div style=\"width: 48%; padding: 10px; border: 1px solid black; overflow-y: auto; height: 400px;\">\n",
    "        {html_text1}\n",
    "    </div>\n",
    "    <div style=\"width: 48%; padding: 10px; border: 1px solid black; overflow-y: auto; height: 400px;\">\n",
    "        {html_text2}\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Display the HTML\n",
    "display(HTML(html_content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"The longer, the better\" is not true in this case\n",
    "\n",
    "I tried approximately maximizing the use of context length, but the result is suboptical considering text output and consumed time.\n",
    "\n",
    "- Relatively the same output quality.\n",
    "- It took 10 minutes (on M4Pro).\n",
    "\n",
    "<img src=\"assets/ollama-max-context.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test variations of `num_predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predicts = [128, 256, 512, 1024, -1, -2]\n",
    "outputs = []\n",
    "\n",
    "for i, num_predict in enumerate(num_predicts):\n",
    "    llm = Ollama(\n",
    "        model=model,\n",
    "        base_url=base_url,\n",
    "        temperature=0.5,\n",
    "        num_ctx=1024*3,\n",
    "        num_predict=num_predict\n",
    "    )\n",
    "    output = llm.invoke(full_prompt)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+++++ OUTPUT 0 +++++\n",
      "\n",
      "('• A conversation is being held with Andre Capathi, previously director of AI '\n",
      " 'at Tesla and OpenAI, Stanford.\\n'\n",
      " '• Neural networks are described as a mathematical abstraction of the brain, '\n",
      " 'using simple math expressions with many knobs (similar to synapses) that can '\n",
      " 'be trained and modified.\\n'\n",
      " '• Despite their simplicity, neural networks exhibit surprising emergent '\n",
      " 'behavior, particularly when trained on complex problems.\\n'\n",
      " '• Capathi suggests that optimizing these networks allows them to discover '\n",
      " 'interesting solutions and \"magical properties\".\\n'\n",
      " '• The conversation touches on the topic of artificial intelligence '\n",
      " 'surpassing human capabilities, with Capathi expressing cautious optimism '\n",
      " \"about the field's potential.\\n\"\n",
      " '• Capathi also discusses')\n",
      "\n",
      "+++++ OUTPUT 1 +++++\n",
      "\n",
      "('• Neural networks are mathematical abstractions of the brain, developed from '\n",
      " 'a simple sequence of matrix multiplies and nonlinearities, with many knobs '\n",
      " '(parameters) that can be adjusted for desired outcomes.\\n'\n",
      " '• The emergence of surprising behaviors in neural networks is due to their '\n",
      " 'ability to optimize complex problems, which can lead to emergent properties '\n",
      " 'and magical-like behavior.\\n'\n",
      " '• Neural networks are trained on large datasets to learn patterns and '\n",
      " 'relationships, and can generate outputs that seem surprising or even '\n",
      " 'creative.\\n'\n",
      " '• Andre Capathi emphasizes the importance of understanding the optimization '\n",
      " 'process behind neural networks, rather than relying solely on analogies to '\n",
      " 'biological systems.\\n'\n",
      " '• Biological neural networks have evolved through a multi-agent self-play '\n",
      " 'process, with evolution favoring individuals with predictive models that '\n",
      " 'help them survive and reproduce.\\n'\n",
      " '• The origin of intelligence or highly complex intelligence is an area of '\n",
      " 'ongoing research and debate, with some arguing that it may be rare or unique '\n",
      " 'to Earth.\\n'\n",
      " '• Andre Capathi is hesitant to assert that humans are uniquely intelligent, '\n",
      " 'but notes that the emergence of technological societies seems rapid and '\n",
      " 'impressive compared to other biological systems.')\n",
      "\n",
      "+++++ OUTPUT 2 +++++\n",
      "\n",
      "('• Neural networks are mathematical abstractions of the brain, inspired by '\n",
      " 'its structure but optimized for a different purpose.\\n'\n",
      " '• The \"knobs\" in neural networks can hold complex representations that '\n",
      " 'capture deep wisdom about data.\\n'\n",
      " '• Large numbers of knobs can lead to surprising emergent behaviors and '\n",
      " '\"magical properties\".\\n'\n",
      " '• Optimization processes used in training neural networks are distinct from '\n",
      " 'those in biological systems, such as the brain.\\n'\n",
      " '• Biological neural networks use multi-agent self-play, evolution, and other '\n",
      " 'mechanisms for survival and reproduction.\\n'\n",
      " '• The origin of intelligence is a remarkable story spanning billions of '\n",
      " 'years, including emergence of life, eukaryotic cells, and complex '\n",
      " 'organisms.\\n'\n",
      " \"• It's unclear whether human-level intelligence is rare or not, but there \"\n",
      " 'may be sparse events where progress was made.\\n'\n",
      " '• There are likely many intelligent alien civilizations in the universe, but '\n",
      " 'we have yet to encounter them.')\n",
      "\n",
      "+++++ OUTPUT 3 +++++\n",
      "\n",
      "('• Neural networks are mathematical abstractions of the brain, originally '\n",
      " 'developed to mimic its structure.\\n'\n",
      " '• They consist of a sequence of matrix multiplies (dot products) with '\n",
      " 'nonlinearities, and knobs that can be trained and modified.\\n'\n",
      " '• Despite being mathematically simple, neural networks exhibit surprising '\n",
      " 'emergent behavior and power.\\n'\n",
      " '• Optimization is key to their performance, allowing them to learn and adapt '\n",
      " 'to complex problems.\\n'\n",
      " '• The representation within the knobs holds deep wisdom about the data '\n",
      " \"they've learned from.\\n\"\n",
      " '• GPT-like models are next-word prediction networks that can solve problems '\n",
      " 'with remarkable consistency.\\n'\n",
      " '• Andre Capathi views neural networks as an \"alien artifact\" created through '\n",
      " 'optimization rather than biological processes.\\n'\n",
      " '• The origin of intelligence is a complex and interesting topic, potentially '\n",
      " 'involving rare events or punctuated equilibrium.\\n'\n",
      " \"• There may be intelligent alien civilizations, but we haven't heard from \"\n",
      " 'them yet due to unknown reasons.')\n",
      "\n",
      "+++++ OUTPUT 4 +++++\n",
      "\n",
      "('• A neural network is a mathematical abstraction of the brain, developed by '\n",
      " 'taking inspiration from it.\\n'\n",
      " \"• It's a simple yet powerful tool for learning and solving problems, with \"\n",
      " 'many knobs (parameters) that can be adjusted for optimal performance.\\n'\n",
      " '• Neural networks have surprising emergent behavior when trained on complex '\n",
      " 'problems, such as next-word prediction in massive data sets.\\n'\n",
      " '• They are used to solve various tasks, including generative models like '\n",
      " 'GPT-like language models.\\n'\n",
      " '• The optimization process that gives rise to neural networks is different '\n",
      " \"from the brain's evolution through multi-agent self-play and natural \"\n",
      " 'selection.\\n'\n",
      " '• Biological neural networks have a complex architecture with many gadgets '\n",
      " 'and gizmos, whereas artificial ones have a simpler mathematical form.\\n'\n",
      " '• The origin of intelligence in humans or other life forms might be a rare '\n",
      " 'event, but not entirely unique.\\n'\n",
      " '• There are likely intelligent alien civilizations out there, given the ease '\n",
      " 'of achieving technological societies on Earth.\\n'\n",
      " '• Understanding how common these civilizations are could reveal insights '\n",
      " 'into the origins of life and technology.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "num_predicts = [128, 256, 512, -1, -2]\n",
    "outputs = []\n",
    "\n",
    "for i, num_predict in enumerate(num_predicts):\n",
    "    llm = ChatOllama(\n",
    "        model=model,\n",
    "        base_url=base_url,\n",
    "        temperature=0.9,\n",
    "        num_ctx=1024*3,\n",
    "        num_predict=num_predict\n",
    "    )\n",
    "    messages = [\n",
    "        { \"role\": \"user\", \"content\": full_prompt},\n",
    "    ]\n",
    "    output = llm.invoke(messages)\n",
    "    outputs.append(output)\n",
    "    print(f\"\\n+++++ OUTPUT {i} +++++\\n\")\n",
    "    pprint(outputs[i].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Output Index  Num Predict  Token Count\n",
      "0             0          128          128\n",
      "1             1          256          213\n",
      "2             2          512          174\n",
      "3             3           -1          179\n",
      "4             4           -2          202\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Output Index': range(len(outputs)),\n",
    "    'Num Predict': num_predicts,\n",
    "    'Token Count': [len(convert_text_to_tokens(output.content)) for output in outputs]\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3p12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
