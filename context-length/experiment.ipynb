{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaLLM as Ollama\n",
    "import tiktoken\n",
    "\n",
    "# QUESTION_TEMPLATE_TXT = \"\"\"Write a concise summary of the following:\n",
    "\n",
    "# \"{text}\"\n",
    "\n",
    "# CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "# REFINE_TEMPLATE_TXT = \"\"\"Your job is to produce a final summary.\n",
    "# We have provided an existing summary up to a certain point: {existing_answer}\n",
    "# We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
    "# ------------\n",
    "# {text}\n",
    "# ------------\n",
    "# Given the new context, refine the original summary.\n",
    "# If the context isn't useful, return the original summary.\"\"\"\n",
    "\n",
    "# MAP_TEMPLATE_TXT = \"\"\"Write a detail summary of this text section in bullet points. Answer just the bullet points, no other text.\n",
    "# Text:\n",
    "# {text}\n",
    "\n",
    "# SUMMARY:\"\"\"\n",
    "    \n",
    "# COMBINE_TEMPLATE_TXT = \"\"\"Combine these summaries into a final summary in bullet points. Answer just the bullet points, no other text.\n",
    "# Text:\n",
    "# {text}\n",
    "\n",
    "# FINAL SUMMARY:\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample-text.txt\", \"r\") as file:\n",
    "    transcript = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"think it's possible that physics has exploits and we should be trying to \"\n",
      " 'find them arranging some kind of a crazy quantum mechanical system that '\n",
      " 'somehow gives you buffer overflow somehow gives you a rounding error in the '\n",
      " 'floating Point synthetic intelligences are kind of like the next stage of '\n",
      " \"development and I don't know where it leads to like at some point I suspect \"\n",
      " 'the universe is some kind of a puzzle these synthetic AIS will uncover that '\n",
      " 'puzzle and solve it the following is a conversation w')\n"
     ]
    }
   ],
   "source": [
    "pprint(transcript[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_splitter(chunk_size: int, overlap_size: int):\n",
    "    return RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size, chunk_overlap=overlap_size)\n",
    "\n",
    "def convert_text_to_tokens(text, encoder=\"gpt-3.5-turbo\"):\n",
    "    enc = tiktoken.encoding_for_model(encoder)\n",
    "    return enc.encode(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare docs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only used 50000 letters from the `transcript` variable, so we can use the same chunk size for all docs\n",
    "chunk_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs:  4\n",
      "Characters counts per document: [12718, 12370, 12612, 12297]\n",
      "Total Characters across 4 documents: 49997\n",
      "Token counts per document: [2478, 2505, 2488, 2395]\n",
      "Total tokens across 4 documents: 9866\n",
      "Word counts per document: [2313, 2332, 2314, 2269]\n",
      "Total words across 4 documents: 9228\n"
     ]
    }
   ],
   "source": [
    "transcript_up_to = 50000\n",
    "\n",
    "chunk_size = 2500 # this is in tokens\n",
    "overlap_size = 0 # this is in tokens\n",
    "# model = \"llama3.2\"\n",
    "# base_url = \"http://localhost:11434\"\n",
    "# temperature = 0.5\n",
    "# # max_context = 1024\n",
    "# num_ctx = 1024*2\n",
    "# num_predict = 128\n",
    "# map_prompt_txt = MAP_TEMPLATE_TXT\n",
    "# combine_prompt_text = COMBINE_TEMPLATE_TXT\n",
    "# question_prompt_txt = QUESTION_TEMPLATE_TXT\n",
    "# refine_prompt_txt = REFINE_TEMPLATE_TXT \n",
    "# chain_type = \"map_reduce\"\n",
    "\n",
    "### CODE\n",
    "\n",
    "docs = [Document(\n",
    "    page_content=transcript[:transcript_up_to],\n",
    "    # metadata={\"source\": url}\n",
    ")]\n",
    "\n",
    "text_splitter = get_text_splitter(chunk_size=chunk_size, overlap_size=overlap_size)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "n_docs = len(split_docs)\n",
    "\n",
    "print(\"number of docs: \", n_docs)\n",
    "\n",
    "# Count characters in each document\n",
    "char_counts = [len(doc.page_content) for doc in split_docs]\n",
    "print(f\"Characters counts per document: {char_counts}\")\n",
    "print(f\"Total Characters across {n_docs} documents: {sum(char_counts)}\")\n",
    "# split_docs\n",
    "  # Using gpt-3.5-turbo tokenizer as closest approximation for llama models\n",
    "token_counts = [len(convert_text_to_tokens(doc.page_content)) for doc in split_docs]\n",
    "print(f\"Token counts per document: {token_counts}\")\n",
    "print(f\"Total tokens across {n_docs} documents: {sum(token_counts)}\")\n",
    "# Count words in each document\n",
    "word_counts = [len(doc.page_content.split()) for doc in split_docs]\n",
    "print(f\"Word counts per document: {word_counts}\")\n",
    "print(f\"Total words across {n_docs} documents: {sum(word_counts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2\"\n",
    "base_url = \"http://localhost:11434\"\n",
    "temperature = 0.5\n",
    "num_ctx = 1024 # number of context length, DEFAULT = 2048\n",
    "# https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#langchain_ollama.llms.OllamaLLM.num_ctx\n",
    "num_predict = 128 # number of tokens to predict, Default: 128, -1 = infinite generation, -2 = fill context\n",
    "# https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#langchain_ollama.llms.OllamaLLM.num_predict\n",
    "# more info: https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#ollamallm\n",
    "\n",
    "llm = Ollama(\n",
    "        model=model,\n",
    "        base_url=base_url,\n",
    "        temperature=temperature,\n",
    "        num_ctx=num_ctx,\n",
    "        num_predict=num_predict,\n",
    "        format='',\n",
    "        verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# if chain_type == \"refine\":\n",
    "#     question_prompt = PromptTemplate(\n",
    "#         template=question_prompt_txt,\n",
    "#         input_variables=[\"text\"]\n",
    "#     )\n",
    "#     refine_prompt = PromptTemplate(\n",
    "#         template=refine_prompt_txt,\n",
    "#         input_variables=[\"existing_answer\", \"text\"]\n",
    "#     )\n",
    "#     chain = load_summarize_chain(\n",
    "#         llm, \n",
    "#         chain_type=\"refine\",\n",
    "#         question_prompt=question_prompt,\n",
    "#         refine_prompt=refine_prompt,\n",
    "#         verbose=True\n",
    "#     )\n",
    "# if chain_type == \"map_reduce\":\n",
    "#     map_prompt = PromptTemplate(\n",
    "#         template=map_prompt_txt,\n",
    "#         input_variables=[\"text\"]\n",
    "#     )\n",
    "\n",
    "#     combine_prompt = PromptTemplate(\n",
    "#         template=combine_prompt_text,\n",
    "#         input_variables=[\"text\"]\n",
    "#     )\n",
    "    \n",
    "#     chain = load_summarize_chain(\n",
    "#         llm, \n",
    "#         chain_type=chain_type,\n",
    "#         map_prompt=map_prompt,\n",
    "#         combine_prompt=combine_prompt,\n",
    "#         verbose=1\n",
    "#     )\n",
    "\n",
    "# output = chain.invoke(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens: 2506\n",
      "\n",
      "('Write a detail summary of this text section in bullet points. Answer just '\n",
      " 'the bullet points, no other text.\\n'\n",
      " 'Text:\\n'\n",
      " \"think it's possible that physics has exploits and we should be trying to \"\n",
      " 'find them arranging some kind of a crazy quantum mechanical system that '\n",
      " 'somehow gives you buffer overflow somehow gives you a rounding error in the '\n",
      " 'floating Point synthetic intelligences are kind of like the next stage of '\n",
      " \"development and I don't know where it leads to like at some point I suspect \"\n",
      " 'the universe is some kind of a puzzle these synthetic AIS will uncover that '\n",
      " 'puzzle and solve it the following is a conversation with Andre capothy '\n",
      " 'previously the director of AI at Tesla and before that at open Ai and '\n",
      " 'Stanford he is one of the greatest scientists engineers and Educators in the '\n",
      " 'history of artificial intelligence this is the Lex Friedman podcast to '\n",
      " \"support it please check out our sponsors and now dear friends here's Andre \"\n",
      " 'capathi what is a neural network and why does it seem to uh do such a '\n",
      " \"surprisingly good job of learning what is a neural network it's a \"\n",
      " \"mathematical abstraction of the brain I would say that's how it was \"\n",
      " \"originally developed at the end of the day it's a mathematical expression \"\n",
      " \"and it's a fairly simple mathematical expression when you get down to it \"\n",
      " \"it's basically a sequence of Matrix multiplies which are really dot products \"\n",
      " \"mathematically and some nonlinearities thrown in and so it's a very simple \"\n",
      " \"mathematical expression and it's got knobs in it many knobs many knobs and \"\n",
      " 'these knobs are Loosely related to basically the synapses in your brain '\n",
      " \"they're trainable they're modifiable and so the idea is like we need to find \"\n",
      " 'the setting of The Knobs that makes the neural nut do whatever you want it '\n",
      " \"to do like classify images and so on and so there's not too much mystery I \"\n",
      " \"would say in it like um you might think that basically don't want to endow \"\n",
      " \"it with too much meaning with respect to the brain and how it works it's \"\n",
      " 'really just a complicated mathematical expression with knobs and those knobs '\n",
      " 'need a proper setting for it to do something uh desirable yeah but poetry is '\n",
      " 'just the collection of letters with spaces but it can make us feel a certain '\n",
      " 'way and in that same way when you get a large number of knobs together '\n",
      " \"whether it's in a inside the brain or inside a computer they seem to they \"\n",
      " \"seem to surprise us with the with their power yeah I think that's fair so \"\n",
      " \"basically I'm underselling it by a lot because you definitely do get very \"\n",
      " \"surprising emergent behaviors out of these neurons when they're large enough \"\n",
      " 'and trained on complicated enough problems like say for example the next uh '\n",
      " 'word prediction in a massive data set from the internet and then these '\n",
      " \"neurons take on a pretty surprising magical properties yeah I think it's \"\n",
      " 'kind of interesting how much you can get out of even very simple '\n",
      " 'mathematical formalism when your brain right now I was talking is it doing '\n",
      " 'next word prediction or is it doing something more interesting well '\n",
      " \"definitely some kind of a generative model that's a gpt-like and prompted by \"\n",
      " \"you um yeah so you're giving me a prompt and I'm kind of like responding to \"\n",
      " 'it in a generative way and by yourself perhaps a little bit like are you '\n",
      " 'adding extra prompts from your own memory inside your head automatically '\n",
      " \"feels like you're referencing some kind of a declarative structure of like \"\n",
      " \"memory and so on and then uh you're putting that together with your prompt \"\n",
      " 'and giving away some messages like how much of what you just said has been '\n",
      " 'said by you before uh nothing basically right no but if you actually look at '\n",
      " \"all the words you've ever said in your life and you do a search you'll \"\n",
      " 'probably said a lot of the same words in the same order before yeah it could '\n",
      " \"be I mean I'm using phrases that are common Etc but I'm remixing it into a \"\n",
      " \"pretty uh sort of unique sentence at the end of the day but you're right \"\n",
      " \"definitely there's like a ton of remixing what you didn't you it's like \"\n",
      " \"Magnus Carlsen said uh I'm I'm rated 2900 whatever which is pretty decent I \"\n",
      " \"think you're talking very uh you're not giving enough credit to neural Nets \"\n",
      " \"here why do they seem to what's your best intuition about this emergent \"\n",
      " \"Behavior I mean it's kind of interesting because I'm simultaneously \"\n",
      " \"underselling them but I also feel like there's an element to which I'm over \"\n",
      " \"like it's actually kind of incredible that you can get so much emergent \"\n",
      " 'magical Behavior out of them despite them being so simple mathematically so '\n",
      " 'I think those are kind of like two surprising statements that are kind of '\n",
      " 'just juxtapose together and I think basically what it is is we are actually '\n",
      " 'fairly good at optimizing these neural Nets and when you give them a hard '\n",
      " 'enough problem they are forced to learn very interesting Solutions in the '\n",
      " 'optimization and those solution basically have these immersion properties '\n",
      " \"that are very interesting there's wisdom and knowledge in the knobs and so \"\n",
      " \"what's this representation that's in the knobs does it make sense to you \"\n",
      " 'intuitively the large number of knobs can hold the representation that '\n",
      " \"captures some deep wisdom about the data it has looked at it's a lot of \"\n",
      " \"knobs it's a lot of knobs and somehow you know so speaking concretely um one \"\n",
      " 'of the neural Nets that people are very excited about right now are are gpts '\n",
      " 'which are basically just next word prediction networks so you consume a '\n",
      " 'sequence of words from the internet and you try to predict the next word and '\n",
      " 'uh once you train these on a large enough data set um they you can basically '\n",
      " 'uh prompt these neural amounts in arbitrary ways and you can ask them to '\n",
      " 'solve problems and they will so you can just tell them you can you can make '\n",
      " \"it look like you're trying to um solve some kind of a mathematical problem \"\n",
      " \"and they will continue what they think is the solution based on what they've \"\n",
      " 'seen on the internet and very often those Solutions look very remarkably '\n",
      " 'consistent look correct potentially do you still think about the brain side '\n",
      " 'of it so as neural Nets is an abstraction or mathematical abstraction of the '\n",
      " 'brain you still draw wisdom from from the biological neural networks or even '\n",
      " \"the bigger question so you're a big fan of biology and biological \"\n",
      " 'computation what impressive thing is biology do doing to you that computers '\n",
      " \"are not yet that Gap I would say I'm definitely on I'm much more hesitant \"\n",
      " 'with the analogies to the brain than I think you would see potentially in '\n",
      " 'the field um and I kind of feel like certainly the way neural network '\n",
      " 'started is everything stemmed from inspiration by the brain but at the end '\n",
      " 'of the day the artifacts that you get after training they are arrived at by '\n",
      " 'a very different optimization process than the optimization process that '\n",
      " 'gave rise to the brain and so I think uh I kind of think of it as a very '\n",
      " \"complicated alien artifact um it's something different I'm not sorry the uh \"\n",
      " \"the neuralness that we're training okay they are complicated uh Alien \"\n",
      " 'artifact uh I do not make analogies to the brain because I think the '\n",
      " 'optimization process that gave rise to it is very different from the brain '\n",
      " 'so there was no multi-agent self-play kind of uh setup uh and evolution it '\n",
      " 'was an optimization that is basically a what amounts to a compression '\n",
      " 'objective on a massive amount of data okay so artificial neural networks are '\n",
      " 'doing compression and biological neural networks are not to survive and '\n",
      " \"they're not really doing any they're they're an agent in a multi-agent \"\n",
      " \"self-place system that's been running for a very very long time that said \"\n",
      " 'Evolution has found that it is very useful to to predict and have a '\n",
      " 'predictive model in the brain and so I think our brain utilizes something '\n",
      " 'that looks like that as a part of it but it has a lot more you know gadgets '\n",
      " 'and gizmos and uh value functions and ancient nuclei that are all trying to '\n",
      " 'like make a survive and reproduce and everything else and the whole thing '\n",
      " \"through embryogenesis is built from a single cell I mean it's just the code \"\n",
      " 'is inside the DNA and it just builds it up like the entire organism yes and '\n",
      " \"like it does it pretty well it should not be possible so there's some \"\n",
      " \"learning going on there's some there's some there's some kind of computation \"\n",
      " \"going through that building process I mean I I don't know where if you were \"\n",
      " 'just to look at the entirety of history of life on Earth where do you think '\n",
      " 'is the most interesting invention is it the origin of life itself is it just '\n",
      " 'jumping to eukaryotes is it mammals is it humans themselves Homo sapiens the '\n",
      " 'the origin of intelligence or highly complex intelligence or or is it all '\n",
      " \"just in continuation the same kind of process certainly I would say it's an \"\n",
      " \"extremely remarkable story that I'm only like briefly learning about \"\n",
      " 'recently all the way from um actually like you almost have to start at the '\n",
      " 'formation of Earth and all of its conditions and the entire solar system and '\n",
      " 'how everything is arranged with Jupiter and Moon and the habitable zone and '\n",
      " \"everything and then you have an active Earth that's turning over material \"\n",
      " \"and um and then you start with a biogenesis and everything and so it's all \"\n",
      " \"like a pretty remarkable story I'm not sure that I can pick like a single \"\n",
      " 'Unique Piece of it that I find most interesting um I guess for me as an '\n",
      " \"artificial intelligence researcher it's probably the last piece we have lots \"\n",
      " 'of animals that uh you know are are not building technological Society but '\n",
      " 'we do and um it seems to have happened very quickly it seems to have '\n",
      " 'happened very recently and uh something very interesting happened there that '\n",
      " \"I don't fully understand I almost understand everything else kind of I think \"\n",
      " \"intuitively uh but I don't understand exactly that part and how quick it was \"\n",
      " 'both explanations would be interesting one is that this is just a '\n",
      " \"continuation of the same kind of process there's nothing special about \"\n",
      " 'humans that would be deeply understanding that would be very interesting '\n",
      " 'that we think of ourselves as special but it was obvious all it was already '\n",
      " 'written in the in the code that you would have greater and greater '\n",
      " 'intelligence emerging and then the other explanation which is something '\n",
      " \"truly special happened something like a rare event whether it's like crazy \"\n",
      " 'rare event like uh Space Odyssey what would it be see if you say like the '\n",
      " 'invention of Fire or the uh as Richard rangham says the beta males deciding '\n",
      " 'a clever way to kill the alpha males by collaborating so just optimizing the '\n",
      " 'collaborations really the multi-agent aspect of the multi-agent and that '\n",
      " 'really being constrained on resources and trying to survive the '\n",
      " 'collaboration aspect is what created the complex intelligence but it seems '\n",
      " \"like it's a natural outgrowth of the evolution process like what could \"\n",
      " 'possibly be a magical thing that happened like a rare thing that would say '\n",
      " 'that humans are actually human level intelligence is actually a really rare '\n",
      " \"thing in the universe yeah I'm hesitant to say that it is rare by the way \"\n",
      " \"but it definitely seems like it's kind of like a punctuated equilibrium \"\n",
      " 'where you have lots of exploration and then you have certain leaps sparse '\n",
      " 'leaps in between so of course like origin of life would be one um you know '\n",
      " 'DNA sex eukaryotic system eukaryotic life um the endosymbiosis event or the '\n",
      " 'archaeon 8 little bacteria you know just the whole thing and then of course '\n",
      " 'emergence of Consciousness and so on so it seems like definitely there are '\n",
      " \"sparse events where mass amount of progress was made but yeah it's kind of \"\n",
      " \"hard to pick one so you don't think humans are unique gotta ask you how many \"\n",
      " 'intelligent aliens civilizations do you think are out there and uh is there '\n",
      " \"intelligence different or similar to ours yeah I've been preoccupied with \"\n",
      " 'this question quite a bit recently uh basically the for me Paradox and just '\n",
      " 'thinking through and and the reason actually that I am very interested in uh '\n",
      " 'the origin of life is fundamentally trying to understand how common it is '\n",
      " 'that there are technological societies out there uh um in space and the more '\n",
      " 'I study it the more I think that um uh there should be quite a few quite a '\n",
      " \"lot why haven't we heard from them because I I agree with you it feels like \"\n",
      " \"I just don't see why what we did here on Earth is so difficult to do yeah \"\n",
      " 'and especially when you get into the details of it I used to think origin of '\n",
      " 'life was very um it was this magical rare event but then you read books like '\n",
      " 'for example McLean um uh the vital question a life ascending Etc and he '\n",
      " 'really gets in and he really makes you believe that this is not that rare '\n",
      " 'basic chemistry you have an active Earth and you have your alkaline Vents '\n",
      " \"and you have lots of alkaline Waters mixing whether it's a devotion and you \"\n",
      " 'have your proton gradients and you have the little porous pockets of these '\n",
      " 'alkaline vents that concentrate chemistry and um basically as he steps '\n",
      " 'through all of these little pieces you start to understand\\n'\n",
      " '\\n'\n",
      " 'SUMMARY:')\n"
     ]
    }
   ],
   "source": [
    "MAP_TEMPLATE_TXT = \"\"\"Write a detail summary of this text section in bullet points. Answer just the bullet points, no other text.\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "SUMMARY:\"\"\"\n",
    "\n",
    "full_prompt = PromptTemplate(\n",
    "    template=MAP_TEMPLATE_TXT,\n",
    "    input_variables=[\"text\"]\n",
    ").format(\n",
    "    text=split_docs[0].page_content\n",
    ")\n",
    "\n",
    "print(\"number of tokens: {}\\n\".format(len(convert_text_to_tokens(full_prompt))))\n",
    "pprint(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1: Overflowing the context\n",
    "\n",
    "**Observe in Ollama that warning is thrown at us, saying \"truncating input prompt\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.invoke(full_prompt)\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Within the context length\n",
    "\n",
    "We will increase the context length of Ollama with `num_ctx` to 3072 (1024*3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2\"\n",
    "base_url = \"http://localhost:11434\"\n",
    "temperature = 0.5\n",
    "num_ctx = 1024*3 # number of context length, DEFAULT = 2048\n",
    "# https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#langchain_ollama.llms.OllamaLLM.num_ctx\n",
    "num_predict = 128 # number of tokens to predict, Default: 128, -1 = infinite generation, -2 = fill context\n",
    "# https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#langchain_ollama.llms.OllamaLLM.num_predict\n",
    "# more info: https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#ollamallm\n",
    "\n",
    "llm = Ollama(\n",
    "        model=model,\n",
    "        base_url=base_url,\n",
    "        temperature=temperature,\n",
    "        num_ctx=num_ctx,\n",
    "        num_predict=num_predict,\n",
    "        format='',\n",
    "        verbose=True\n",
    ")\n",
    "\n",
    "output = llm.invoke(full_prompt)\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for i in range(5):\n",
    "    print(\"\\n======================= Output {} ==========================\".format(i+1))\n",
    "    _output = llm.invoke(full_prompt)\n",
    "    pprint(_output)\n",
    "    outputs.append(_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3: Increase prediction length\n",
    "To increase the chance of AI completing its answer, let's increase the prediction length with `num_pred` to 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2\"\n",
    "base_url = \"http://localhost:11434\"\n",
    "temperature = 0.5\n",
    "num_ctx = 1024*3 # number of context length, DEFAULT = 2048\n",
    "# https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#langchain_ollama.llms.OllamaLLM.num_ctx\n",
    "num_predict = 1024 # number of tokens to predict, Default: 128, -1 = infinite generation, -2 = fill context\n",
    "# https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#langchain_ollama.llms.OllamaLLM.num_predict\n",
    "# more info: https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#ollamallm\n",
    "\n",
    "llm = Ollama(\n",
    "        model=model,\n",
    "        base_url=base_url,\n",
    "        temperature=temperature,\n",
    "        num_ctx=num_ctx,\n",
    "        num_predict=num_predict,\n",
    "        format='',\n",
    "        verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.invoke(full_prompt)\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with GPT-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"secret/openai_api_key.txt\", \"r\") as file:\n",
    "    openai_api_key = file.read()\n",
    "\n",
    "# openai.api_key = openai_api_key\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    api_key=openai_api_key,\n",
    ")\n",
    "\n",
    "output_gpt = chat_llm.invoke(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output_gpt.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import markdown\n",
    "\n",
    "# Convert Markdown to HTML\n",
    "html_text1 = markdown.markdown(\"# Ollama \\n\"+output.replace(\"•\",\"-\"))\n",
    "html_text2 = markdown.markdown(\"# GPT-4o-mini \\n\"+output_gpt.content)\n",
    "\n",
    "# Combine into side-by-side layout\n",
    "html_content = f\"\"\"\n",
    "<div style=\"display: flex; justify-content: space-between; width: 100%;\">\n",
    "    <div style=\"width: 48%; padding: 10px; border: 1px solid black; overflow-y: auto; height: 400px;\">\n",
    "        {html_text1}\n",
    "    </div>\n",
    "    <div style=\"width: 48%; padding: 10px; border: 1px solid black; overflow-y: auto; height: 400px;\">\n",
    "        {html_text2}\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Display the HTML\n",
    "display(HTML(html_content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"The longer, the better\" is not true in this case\n",
    "\n",
    "I tried approximately maximizing the use of context length, but the result is suboptical considering text output and consumed time.\n",
    "\n",
    "- Relatively the same output quality.\n",
    "- It took 10 minutes (on M4Pro).\n",
    "\n",
    "<img src=\"assets/ollama-max-context.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test variations of `num_predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predicts = [128, 256, 512, 1024, -1, -2]\n",
    "outputs = []\n",
    "\n",
    "for i, num_predict in enumerate(num_predicts):\n",
    "    llm = Ollama(\n",
    "        model=model,\n",
    "        base_url=base_url,\n",
    "        temperature=0.5,\n",
    "        num_ctx=1024*3,\n",
    "        num_predict=num_predict\n",
    "    )\n",
    "    output = llm.invoke(full_prompt)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+++++ OUTPUT 0 +++++\n",
      "\n",
      "('• A neural network is a mathematical abstraction of the brain, originally '\n",
      " 'developed at the end of the day as a simple mathematical expression with '\n",
      " 'knobs that can be adjusted to make it learn and perform tasks.\\n'\n",
      " '\\n'\n",
      " '• Despite being mathematically simple, neural networks exhibit surprising '\n",
      " 'emergent behavior, such as next-word prediction in massive data sets.\\n'\n",
      " '\\n'\n",
      " '• The optimization process that gives rise to neural networks is different '\n",
      " \"from the brain's optimization process, which involved multi-agent self-play \"\n",
      " 'and evolution over millions of years.\\n'\n",
      " '\\n'\n",
      " '• Biological neural networks are still not fully understood, but their '\n",
      " 'optimization process is thought to be constrained by survival and '\n",
      " 'reproduction needs.\\n'\n",
      " '\\n'\n",
      " '• The origin of life itself')\n",
      "\n",
      "+++++ OUTPUT 1 +++++\n",
      "\n",
      "('• Andre Capathi explains that neural networks are mathematical abstractions '\n",
      " 'of the brain, originally developed as a simple mathematical expression with '\n",
      " 'many \"knobs\" that can be adjusted for optimal performance.\\n'\n",
      " '• He notes that neural networks exhibit surprising emergent behaviors '\n",
      " \"despite being mathematically simple, and attributes this to humans' ability \"\n",
      " 'to optimize them effectively for complex problems.\\n'\n",
      " '• Capathi highlights GPT-like models, which can solve problems in arbitrary '\n",
      " 'ways by prompting them with user-defined inputs.\\n'\n",
      " '• Despite drawing inspiration from biological neural networks, Capathi '\n",
      " 'believes that artificial neural networks are fundamentally different due to '\n",
      " \"their optimization process being distinct from the brain's multi-agent \"\n",
      " 'self-play and evolution-based mechanisms.\\n'\n",
      " '• He considers the origin of life on Earth an extremely remarkable story, '\n",
      " \"but doesn't think humans are uniquely intelligent or special, instead \"\n",
      " 'believing in a punctuated equilibrium of exploration and sparse leaps in '\n",
      " 'technological progress.\\n'\n",
      " '• Capathi speculates that there may be many intelligent alien civilizations '\n",
      " \"out there, wondering why we haven't heard from them despite the relative \"\n",
      " 'difficulty of establishing technological societies on Earth.')\n",
      "\n",
      "+++++ OUTPUT 2 +++++\n",
      "\n",
      "('• A conversation with Andre Capathi about neural networks, AI, and their '\n",
      " 'ability to learn and perform surprising tasks.\\n'\n",
      " '• Neural networks are mathematically simple but can produce complex emergent '\n",
      " 'behavior when trained on large amounts of data.\\n'\n",
      " '• The power of neural networks comes from the optimization process that '\n",
      " 'finds the optimal settings for the knobs, which can lead to remarkable '\n",
      " 'results.\\n'\n",
      " '• Capathi highlights the importance of optimizing neural networks and notes '\n",
      " 'that the representation in the knobs is not fully understood intuitively.\\n'\n",
      " '• He discusses the development of GPT-like models, which are next-word '\n",
      " 'prediction networks trained on large datasets and can be prompted to solve '\n",
      " 'problems.\\n'\n",
      " '• Capathi emphasizes that biological neural networks have a unique '\n",
      " 'optimization process that differs from artificial neural networks.\\n'\n",
      " '• The origin of life and the emergence of intelligent species is considered '\n",
      " 'an extraordinary event that may not be rare in the universe.\\n'\n",
      " '• Andre Capathi wonders if there are many intelligent alien civilizations '\n",
      " \"out there, but notes that the reason we haven't heard from them is unclear.\\n\"\n",
      " '• He suggests that the conditions on Earth and the chemical processes '\n",
      " \"involved in life's origin might be more common than previously thought.\")\n",
      "\n",
      "+++++ OUTPUT 3 +++++\n",
      "\n",
      "('• A neural network is a mathematical abstraction of the brain, developed at '\n",
      " \"the end of the day, it's a simple mathematical expression with knobs \"\n",
      " '(parameters) that can be adjusted to make it learn and perform tasks.\\n'\n",
      " '• The emergence of surprising behaviors in neural networks arises from '\n",
      " 'optimizing these complex systems on large amounts of data.\\n'\n",
      " '• GPT-like models are next-word prediction networks trained on massive '\n",
      " 'datasets, which can solve problems by generating text based on patterns '\n",
      " 'learned from the internet.\\n'\n",
      " '• Andre Caputhi emphasizes that neural networks are an \"alien artifact\" '\n",
      " 'different from biological brain processes, as their optimization is '\n",
      " 'fundamentally different from evolution.\\n'\n",
      " '• The origin of life, emergence of consciousness, and complex intelligence '\n",
      " 'might be sparse events in the universe, punctuated by periods of rapid '\n",
      " 'progress.\\n'\n",
      " \"• It's difficult to determine if humans are unique, but many believe that \"\n",
      " 'intelligent alien civilizations could exist and potentially communicate with '\n",
      " 'us.\\n'\n",
      " '• The search for extraterrestrial intelligence (SETI) aims to understand why '\n",
      " \"we haven't encountered other technological societies despite being capable \"\n",
      " 'of it.')\n",
      "\n",
      "+++++ OUTPUT 4 +++++\n",
      "\n",
      "('• A conversation between Andre Capathi, former AI director at Tesla and '\n",
      " 'OpenAI, and Lex Friedman discusses neural networks.\\n'\n",
      " '• Neural networks are a mathematical abstraction of the brain and consist of '\n",
      " 'simple mathematical expressions with many knobs that can be adjusted for '\n",
      " 'optimal performance.\\n'\n",
      " '• The knobs in neural networks are trainable and modifiable, allowing them '\n",
      " 'to learn and adapt to complex problems like next word prediction in massive '\n",
      " 'data sets.\\n'\n",
      " '• Capathi believes that surprising emergent behaviors can arise from these '\n",
      " 'simple mathematical formalism when trained on complicated enough problems.\\n'\n",
      " '• Neurons in large neural networks exhibit magical properties, including '\n",
      " 'generative models that resemble GPT-like systems.\\n'\n",
      " '• Despite their simplicity mathematically, neural networks can produce '\n",
      " 'surprisingly powerful and interesting results, particularly with proper '\n",
      " 'optimization.\\n'\n",
      " '• Capathi emphasizes the importance of recognizing that artificial neural '\n",
      " 'networks are \"complicated alien artifacts\" rather than direct analogs to '\n",
      " 'biological brains.\\n'\n",
      " '• He notes that our understanding of intelligence is still evolving, but '\n",
      " \"it's uncertain if humans possess truly unique or special intelligence in the \"\n",
      " 'universe.\\n'\n",
      " '• Discussing the possibility of intelligent alien civilizations, Capathi '\n",
      " \"thinks there may be many out there but hasn't heard from them due to the \"\n",
      " 'difficulty in establishing communication and technological advancements.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "num_predicts = [128, 256, 512, -1, -2]\n",
    "outputs = []\n",
    "\n",
    "for i, num_predict in enumerate(num_predicts):\n",
    "    llm = ChatOllama(\n",
    "        model=model,\n",
    "        base_url=base_url,\n",
    "        temperature=0.9,\n",
    "        num_ctx=1024*3,\n",
    "        num_predict=num_predict\n",
    "    )\n",
    "    messages = [\n",
    "        { \"role\": \"user\", \"content\": full_prompt},\n",
    "    ]\n",
    "    output = llm.invoke(messages)\n",
    "    outputs.append(output)\n",
    "    print(f\"\\n+++++ OUTPUT {i} +++++\\n\")\n",
    "    pprint(outputs[i].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Output Index  Num Predict  Token Count\n",
      "0             0          128          128\n",
      "1             1          256          209\n",
      "2             2          512          226\n",
      "3             3           -1          208\n",
      "4             4           -2          242\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Output Index': range(len(outputs)),\n",
    "    'Num Predict': num_predicts,\n",
    "    'Token Count': [len(convert_text_to_tokens(output.content)) for output in outputs]\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3p12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
